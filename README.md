# ğŸ“± CodeGen (Code Generator) - Ollama

Projeto FastAPI para gerar **cÃ³digos de telas mobile** usando modelos de linguagem (LLMs) locais via **[Ollama](https://ollama.com/)**. A API permite descrever uma tela e obter mÃºltiplas versÃµes de cÃ³digo gerado automaticamente â€” que sÃ£o salvas como arquivos `.md`.

---

## ğŸš€ Tecnologias usadas

- [FastAPI](https://fastapi.tiangolo.com/)
- [Ollama](https://ollama.com/) (modelos LLM locais como `codellama`, `llama3`, `mistral`, etc.)
- Python 3.9+
- httpx

---

## ğŸ“‚ Estrutura do projeto

```bash
mobile-codegen/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py       # API FastAPI
â”‚   â”œâ”€â”€ schemas.py    # Modelos Pydantic (request/response)
â”‚   â”œâ”€â”€ generator.py  # ComunicaÃ§Ã£o com Ollama
â”œâ”€â”€ outputs/          # CÃ³digos gerados em .md
â”œâ”€â”€ requirements.txt  # DependÃªncias
â”œâ”€â”€ .env              # (opcional) configuraÃ§Ãµes
â””â”€â”€ README.md         # Este arquivo
```

---

## âš™ï¸ InstalaÃ§Ã£o

1. **Clone o repositÃ³rio:**

```bash
git clone https://github.com/seu-usuario/mobile-codegen.git
cd mobile-codegen
```

2. **Crie e ative um ambiente virtual (recomendado):**

```bash
python -m venv venv
source venv/bin/activate  # Linux/macOS
venv\Scripts\activate     # Windows
```

3. **Instale as dependÃªncias:**

```bash
pip install -r requirements.txt
```

4. **Certifique-se de que o Ollama esteja instalado e rodando:**

```bash
ollama run codellama
```

> âš ï¸ O modelo deve estar disponÃ­vel localmente. Substitua por outro se preferir: ``llama3``, ``mistral``, etc.

---

## â–¶ï¸ ExecuÃ§Ã£o

Rode o servidor FastAPI com:

```bash
uvicorn app.main:app --reload
```

Acesse a interface Swagger:

```bash
http://localhost:8000/docs
```

---

## ğŸ“¥ Exemplo de requisiÃ§Ã£o

```json
POST /gerar-tela
Content-Type: application/json

{
  "descricao": "Tela de login com e-mail e senha",
  "framework": "Flutter",
  "linguagem": "Dart",
  "modelo": "codellama",
  "repeticoes": 2
}
```

---

## ğŸ“¤ O que a API retorna

```json
{
  "codigos": [
    "...cÃ³digo 1...",
    "...cÃ³digo 2..."
  ]
}
```

AlÃ©m disso, os cÃ³digos sÃ£o salvos automaticamente em arquivos `.md` na pasta `outputs/`, com nomes baseados em data e hash da descriÃ§Ã£o.

Exemplo:

```bash
outputs/
â”œâ”€â”€ 20250803-153210_ab12cd_1.md
â”œâ”€â”€ 20250803-153210_ab12cd_2.md
```

---

## ğŸ“Œ Modelos suportados

VocÃª pode usar qualquer modelo suportado pelo Ollama, incluindo:
- codellama
- llama3
- mistral
- phi3
- gemma
- e outros...

Verifique os modelos instalados com:

```bash
ollama list
```

---

## âœ… To-do (ideias futuras)
âœ… Escolha do modelo LLM por requisiÃ§Ã£o

âœ… Suporte a mÃºltiplas repetiÃ§Ãµes

âœ… GeraÃ§Ã£o de arquivos .md com cada variaÃ§Ã£o

â³ IntegraÃ§Ã£o com frontend de visualizaÃ§Ã£o

â³ Suporte a geraÃ§Ã£o por lote de telas

---

## ğŸ§  Requisitos
- Python 3.9+
- Ollama instalado e funcional
- Modelos carregados localmente (ollama run <modelo>)

---

## ğŸ“ƒ LicenÃ§a
Este projeto estÃ¡ sob a licenÃ§a MIT.